{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç AI Product Photo Detector ‚Äî Training Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nolancacheux/AI-Product-Photo-Detector/blob/main/notebooks/train_colab.ipynb)\n",
    "\n",
    "Train an **EfficientNet-B0** binary classifier to distinguish **real product photos** from **AI-generated images**.\n",
    "\n",
    "## üìã What this notebook does:\n",
    "1. **Setup** ‚Äî Install dependencies, authenticate with GCS\n",
    "2. **Data** ‚Äî Download processed dataset from GCS bucket\n",
    "3. **Training** ‚Äî Train EfficientNet-B0 with the project's training pipeline\n",
    "4. **Evaluation** ‚Äî Compute accuracy, F1, precision, recall\n",
    "5. **Visualization** ‚Äî Training curves, confusion matrix, Grad-CAM heatmaps\n",
    "6. **Export** ‚Äî Save model checkpoint to GCS\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Requirements:** GPU runtime recommended. Go to `Runtime > Change runtime type > T4 GPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üõ†Ô∏è Setup\n",
    "\n",
    "Clone the repository, install dependencies, and authenticate with Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone --depth 1 https://github.com/nolancacheux/AI-Product-Photo-Detector.git\n",
    "%cd AI-Product-Photo-Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision timm pillow scikit-learn matplotlib tqdm \\\n",
    "    google-cloud-storage pyyaml structlog grad-cam numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "print(\"‚úÖ GCS authentication successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from google.cloud import storage\n",
    "import yaml\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ‚öôÔ∏è Configuration\n",
    "\n",
    "Load training configuration from `configs/train_config.yaml` with Colab-specific overrides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Configuration\n",
    "GCP_PROJECT = \"ai-product-detector-487013\"\n",
    "GCS_BUCKET = \"ai-product-detector-487013\"\n",
    "GCS_DATA_PREFIX = \"data/processed\"\n",
    "GCS_MODEL_PATH = \"models/best_model.pt\"\n",
    "\n",
    "# Load base config from repo\n",
    "with open(\"configs/train_config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Colab-specific overrides\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    \"train_dir\": \"data/processed/train\",\n",
    "    \"val_dir\": \"data/processed/val\",\n",
    "    \"test_dir\": \"data/processed/test\",\n",
    "    \"image_size\": config.get(\"data\", {}).get(\"image_size\", 224),\n",
    "    \"batch_size\": config.get(\"data\", {}).get(\"batch_size\", 64),\n",
    "    \"num_workers\": 2,  # Colab limitation\n",
    "\n",
    "    # Model\n",
    "    \"model_name\": config.get(\"model\", {}).get(\"name\", \"efficientnet_b0\"),\n",
    "    \"pretrained\": config.get(\"model\", {}).get(\"pretrained\", True),\n",
    "    \"dropout\": config.get(\"model\", {}).get(\"dropout\", 0.3),\n",
    "\n",
    "    # Training\n",
    "    \"epochs\": config.get(\"training\", {}).get(\"epochs\", 15),\n",
    "    \"learning_rate\": config.get(\"training\", {}).get(\"learning_rate\", 0.001),\n",
    "    \"weight_decay\": config.get(\"training\", {}).get(\"weight_decay\", 0.0001),\n",
    "    \"patience\": config.get(\"training\", {}).get(\"early_stopping_patience\", 5),\n",
    "\n",
    "    # Output\n",
    "    \"output_dir\": \"./training_output\",\n",
    "    \"model_filename\": \"best_model.pt\",\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
    "print(\"üìã Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üì¶ Data\n",
    "\n",
    "Download processed dataset from GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_gcs(bucket_name: str, gcs_prefix: str, local_dir: str) -> int:\n",
    "    \"\"\"Download directory from GCS to local filesystem.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name: GCS bucket name.\n",
    "        gcs_prefix: Prefix path in GCS.\n",
    "        local_dir: Local directory to download to.\n",
    "        \n",
    "    Returns:\n",
    "        Number of files downloaded.\n",
    "    \"\"\"\n",
    "    client = storage.Client(project=GCP_PROJECT)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=gcs_prefix))\n",
    "    \n",
    "    downloaded = 0\n",
    "    for blob in tqdm(blobs, desc=f\"Downloading {gcs_prefix}\"):\n",
    "        if blob.name.endswith(\"/\"):\n",
    "            continue  # Skip directories\n",
    "            \n",
    "        # Create local path\n",
    "        relative_path = blob.name[len(gcs_prefix):].lstrip(\"/\")\n",
    "        local_path = Path(local_dir) / relative_path\n",
    "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Download\n",
    "        blob.download_to_filename(str(local_path))\n",
    "        downloaded += 1\n",
    "        \n",
    "    return downloaded\n",
    "\n",
    "\n",
    "# Download training data\n",
    "print(f\"üì• Downloading data from gs://{GCS_BUCKET}/{GCS_DATA_PREFIX}/...\")\n",
    "n_files = download_from_gcs(GCS_BUCKET, GCS_DATA_PREFIX, \"data/processed\")\n",
    "print(f\"‚úÖ Downloaded {n_files} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data structure\n",
    "def count_images(directory: str) -> dict:\n",
    "    \"\"\"Count images in each class subdirectory.\"\"\"\n",
    "    counts = {}\n",
    "    base_path = Path(directory)\n",
    "    if not base_path.exists():\n",
    "        return counts\n",
    "    for class_dir in base_path.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            n = len(list(class_dir.glob(\"*.[jJpP][pPnN][gG]*\")))\n",
    "            counts[class_dir.name] = n\n",
    "    return counts\n",
    "\n",
    "print(\"\\nüìä Dataset Statistics:\")\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    split_dir = f\"data/processed/{split}\"\n",
    "    counts = count_images(split_dir)\n",
    "    total = sum(counts.values())\n",
    "    print(f\"  {split:>5}: {total:,} images\", end=\"\")\n",
    "    if counts:\n",
    "        print(f\" ({', '.join(f'{k}: {v}' for k, v in counts.items())})\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class (matches src/training/dataset.py)\n",
    "class AIProductDataset(Dataset):\n",
    "    \"\"\"Dataset for AI vs Real product image classification.\n",
    "    \n",
    "    Directory structure:\n",
    "        data_dir/\n",
    "        ‚îú‚îÄ‚îÄ real/\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg\n",
    "        ‚îî‚îÄ‚îÄ ai_generated/\n",
    "            ‚îî‚îÄ‚îÄ *.jpg\n",
    "    \n",
    "    Labels: 0 = Real, 1 = AI-generated\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str, transform=None, image_size: int = 224):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform or self._default_transform()\n",
    "        self.samples = []\n",
    "        self._load_samples()\n",
    "\n",
    "    def _default_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def _load_samples(self):\n",
    "        extensions = [\".jpg\", \".jpeg\", \".png\", \".webp\"]\n",
    "        \n",
    "        # Real images (label = 0)\n",
    "        real_dir = self.data_dir / \"real\"\n",
    "        if real_dir.exists():\n",
    "            for img_path in real_dir.iterdir():\n",
    "                if img_path.suffix.lower() in extensions:\n",
    "                    self.samples.append((img_path, 0))\n",
    "\n",
    "        # AI-generated images (label = 1)\n",
    "        ai_dir = self.data_dir / \"ai_generated\"\n",
    "        if ai_dir.exists():\n",
    "            for img_path in ai_dir.iterdir():\n",
    "                if img_path.suffix.lower() in extensions:\n",
    "                    self.samples.append((img_path, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms (matches src/training/augmentation.py)\n",
    "IMG_SIZE = CONFIG[\"image_size\"]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AIProductDataset(CONFIG[\"train_dir\"], transform=train_transform)\n",
    "val_dataset = AIProductDataset(CONFIG[\"val_dir\"], transform=val_transform)\n",
    "test_dataset = AIProductDataset(CONFIG[\"test_dir\"], transform=val_transform)\n",
    "\n",
    "print(f\"\\nüìä Datasets loaded:\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "print(f\"   Val:   {len(val_dataset):,} samples\")\n",
    "print(f\"   Test:  {len(test_dataset):,} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=CONFIG[\"num_workers\"], \n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    num_workers=CONFIG[\"num_workers\"], \n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    num_workers=CONFIG[\"num_workers\"], \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "def show_samples(loader, n_per_class=4):\n",
    "    \"\"\"Display sample images from each class.\"\"\"\n",
    "    fig, axes = plt.subplots(2, n_per_class, figsize=(n_per_class * 3, 6))\n",
    "    fig.suptitle(\"Sample Images\\nTop: Real | Bottom: AI-Generated\", fontsize=14)\n",
    "\n",
    "    inv_normalize = transforms.Compose([\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),\n",
    "    ])\n",
    "\n",
    "    real_shown, fake_shown = 0, 0\n",
    "    for images, labels in loader:\n",
    "        for img, lbl in zip(images, labels):\n",
    "            img_show = inv_normalize(img).permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "            if lbl.item() == 0 and real_shown < n_per_class:\n",
    "                axes[0][real_shown].imshow(img_show)\n",
    "                axes[0][real_shown].axis(\"off\")\n",
    "                axes[0][real_shown].set_title(\"Real\")\n",
    "                real_shown += 1\n",
    "            elif lbl.item() == 1 and fake_shown < n_per_class:\n",
    "                axes[1][fake_shown].imshow(img_show)\n",
    "                axes[1][fake_shown].axis(\"off\")\n",
    "                axes[1][fake_shown].set_title(\"AI-Generated\")\n",
    "                fake_shown += 1\n",
    "            if real_shown >= n_per_class and fake_shown >= n_per_class:\n",
    "                break\n",
    "        if real_shown >= n_per_class and fake_shown >= n_per_class:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/sample_images.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "show_samples(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üß† Model\n",
    "\n",
    "EfficientNet-B0 with custom binary classification head (matches `src/training/model.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIImageDetector(nn.Module):\n",
    "    \"\"\"EfficientNet-based binary classifier for AI image detection.\n",
    "    \n",
    "    This class is redefined here to match src/training/model.py for standalone\n",
    "    notebook execution in Colab without requiring the full repo in PYTHONPATH.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"efficientnet_b0\",\n",
    "        pretrained: bool = True,\n",
    "        dropout: float = 0.3,\n",
    "        freeze_backbone: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,  # Remove classifier head\n",
    "        )\n",
    "\n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Get feature dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            features = self.backbone(dummy)\n",
    "            feature_dim = features.shape[1]\n",
    "\n",
    "        # Classification head (outputs raw logits for BCEWithLogitsLoss)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return torch.sigmoid(self.forward(x))\n",
    "\n",
    "    def get_num_trainable_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def get_num_total_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = AIImageDetector(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    pretrained=CONFIG[\"pretrained\"],\n",
    "    dropout=CONFIG[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nüß† Model: {CONFIG['model_name']}\")\n",
    "print(f\"   Feature dimension: {model.feature_dim}\")\n",
    "print(f\"   Total parameters: {model.get_num_total_params():,}\")\n",
    "print(f\"   Trainable parameters: {model.get_num_trainable_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üèãÔ∏è Training\n",
    "\n",
    "Training loop with early stopping and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, optimizer, scheduler\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    ")\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG[\"epochs\"])\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        predicted = (outputs > 0.0).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model and compute all metrics.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels_gpu = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels_gpu)\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        probs = torch.sigmoid(outputs).cpu()\n",
    "        predicted = (outputs > 0.0).float()\n",
    "        correct += (predicted == labels_gpu).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # For precision/recall\n",
    "        tp += ((predicted == 1) & (labels_gpu == 1)).sum().item()\n",
    "        fp += ((predicted == 1) & (labels_gpu == 0)).sum().item()\n",
    "        fn += ((predicted == 0) & (labels_gpu == 1)).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.squeeze().cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_probs.extend(probs.squeeze().numpy())\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": total_loss / total,\n",
    "        \"accuracy\": correct / total,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "    return metrics, np.array(all_labels), np.array(all_preds), np.array(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "history = {\n",
    "    \"train_loss\": [], \"train_acc\": [],\n",
    "    \"val_loss\": [], \"val_acc\": [],\n",
    "    \"val_precision\": [], \"val_recall\": [], \"val_f1\": [],\n",
    "    \"lr\": []\n",
    "}\n",
    "\n",
    "print(f\"\\nüèãÔ∏è Training for {CONFIG['epochs']} epochs...\")\n",
    "print(f\"{'Epoch':>5} | {'Train Loss':>10} | {'Train Acc':>9} | {'Val Loss':>8} | {'Val Acc':>7} | {'Val F1':>6} | {'LR':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    t0 = time.time()\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_metrics, _, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "    history[\"val_acc\"].append(val_metrics[\"accuracy\"])\n",
    "    history[\"val_precision\"].append(val_metrics[\"precision\"])\n",
    "    history[\"val_recall\"].append(val_metrics[\"recall\"])\n",
    "    history[\"val_f1\"].append(val_metrics[\"f1\"])\n",
    "    history[\"lr\"].append(lr)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    marker = \"\"\n",
    "\n",
    "    if val_metrics[\"accuracy\"] > best_val_acc:\n",
    "        best_val_acc = val_metrics[\"accuracy\"]\n",
    "        patience_counter = 0\n",
    "        marker = \" ‚≠ê best\"\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"val_accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"best_val_accuracy\": best_val_acc,\n",
    "            \"config\": CONFIG,\n",
    "            \"history\": history,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{CONFIG['output_dir']}/{CONFIG['model_filename']}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    print(f\"{epoch+1:>5} | {train_loss:>10.4f} | {train_acc:>8.1%} | {val_metrics['loss']:>8.4f} | {val_metrics['accuracy']:>6.1%} | {val_metrics['f1']:>6.3f} | {lr:>10.2e} | {elapsed:.0f}s{marker}\")\n",
    "\n",
    "    if patience_counter >= CONFIG[\"patience\"]:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch+1} (no improvement for {CONFIG['patience']} epochs)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üìä Evaluation\n",
    "\n",
    "Comprehensive evaluation on test set with all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint = torch.load(f\"{CONFIG['output_dir']}/{CONFIG['model_filename']}\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics, y_true, y_pred, y_probs = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k:>12}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Real\", \"AI-Generated\"], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìà Visualization\n",
    "\n",
    "Training curves, confusion matrix, ROC curve, and Grad-CAM heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history[\"train_loss\"], label=\"Train\", linewidth=2)\n",
    "axes[0, 0].plot(history[\"val_loss\"], label=\"Validation\", linewidth=2)\n",
    "axes[0, 0].set_title(\"Loss\", fontsize=13)\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history[\"train_acc\"], label=\"Train\", linewidth=2)\n",
    "axes[0, 1].plot(history[\"val_acc\"], label=\"Validation\", linewidth=2)\n",
    "axes[0, 1].set_title(\"Accuracy\", fontsize=13)\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "axes[1, 0].plot(history[\"val_precision\"], label=\"Precision\", linewidth=2)\n",
    "axes[1, 0].plot(history[\"val_recall\"], label=\"Recall\", linewidth=2)\n",
    "axes[1, 0].plot(history[\"val_f1\"], label=\"F1\", linewidth=2)\n",
    "axes[1, 0].set_title(\"Validation Metrics\", fontsize=13)\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history[\"lr\"], linewidth=2, color=\"purple\")\n",
    "axes[1, 1].set_title(\"Learning Rate Schedule\", fontsize=13)\n",
    "axes[1, 1].set_xlabel(\"Epoch\")\n",
    "axes[1, 1].set_ylabel(\"Learning Rate\")\n",
    "axes[1, 1].set_yscale(\"log\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['output_dir']}/training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix + ROC curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "im = axes[0].imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "axes[0].set_title(\"Confusion Matrix\", fontsize=14)\n",
    "axes[0].set_xlabel(\"Predicted\", fontsize=12)\n",
    "axes[0].set_ylabel(\"True\", fontsize=12)\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels([\"Real\", \"AI-Generated\"])\n",
    "axes[0].set_yticklabels([\"Real\", \"AI-Generated\"])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        color = \"white\" if cm[i, j] > cm.max() / 2 else \"black\"\n",
    "        axes[0].text(j, i, f\"{cm[i, j]}\\n({cm[i, j]/cm.sum()*100:.1f}%)\", \n",
    "                     ha=\"center\", va=\"center\", color=color, fontsize=14)\n",
    "fig.colorbar(im, ax=axes[0])\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "axes[1].plot(fpr, tpr, linewidth=2, color=\"darkorange\", label=f\"ROC (AUC = {roc_auc:.3f})\")\n",
    "axes[1].plot([0, 1], [0, 1], \"k--\", alpha=0.5, label=\"Random\")\n",
    "axes[1].fill_between(fpr, tpr, alpha=0.2, color=\"darkorange\")\n",
    "axes[1].set_title(\"ROC Curve\", fontsize=14)\n",
    "axes[1].set_xlabel(\"False Positive Rate\", fontsize=12)\n",
    "axes[1].set_ylabel(\"True Positive Rate\", fontsize=12)\n",
    "axes[1].legend(fontsize=11, loc=\"lower right\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{CONFIG['output_dir']}/evaluation.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä AUC-ROC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM visualization\n",
    "try:\n",
    "    from pytorch_grad_cam import GradCAM\n",
    "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "    from pytorch_grad_cam.utils.model_targets import BinaryClassifierOutputTarget\n",
    "\n",
    "    # Get the last convolutional layer from backbone\n",
    "    target_layers = [model.backbone.conv_head]  # EfficientNet's last conv layer\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "\n",
    "    # Inverse normalization for visualization\n",
    "    inv_normalize = transforms.Compose([\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "        transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),\n",
    "    ])\n",
    "\n",
    "    # Get sample images\n",
    "    n_samples = 4\n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=(n_samples * 4, 8))\n",
    "    fig.suptitle(\"Grad-CAM Visualization\\nTop: Original | Bottom: Attention Heatmap\", fontsize=14)\n",
    "\n",
    "    shown = 0\n",
    "    for images, labels in test_loader:\n",
    "        for i, (img, lbl) in enumerate(zip(images, labels)):\n",
    "            if shown >= n_samples:\n",
    "                break\n",
    "                \n",
    "            # Prepare input\n",
    "            input_tensor = img.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                prob = torch.sigmoid(model(input_tensor)).item()\n",
    "            pred_label = \"AI-Generated\" if prob > 0.5 else \"Real\"\n",
    "            true_label = \"AI-Generated\" if lbl.item() == 1 else \"Real\"\n",
    "            \n",
    "            # Generate CAM\n",
    "            targets = [BinaryClassifierOutputTarget(1)]  # Target: AI-generated class\n",
    "            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "            grayscale_cam = grayscale_cam[0, :]\n",
    "            \n",
    "            # Prepare image for visualization\n",
    "            rgb_img = inv_normalize(img).permute(1, 2, 0).clamp(0, 1).numpy()\n",
    "            visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "            \n",
    "            # Display\n",
    "            axes[0, shown].imshow(rgb_img)\n",
    "            axes[0, shown].axis(\"off\")\n",
    "            axes[0, shown].set_title(f\"True: {true_label}\", fontsize=11)\n",
    "            \n",
    "            axes[1, shown].imshow(visualization)\n",
    "            axes[1, shown].axis(\"off\")\n",
    "            correct = \"‚úì\" if (prob > 0.5) == (lbl.item() == 1) else \"‚úó\"\n",
    "            axes[1, shown].set_title(f\"Pred: {pred_label} ({prob:.2f}) {correct}\", fontsize=11)\n",
    "            \n",
    "            shown += 1\n",
    "            \n",
    "        if shown >= n_samples:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{CONFIG['output_dir']}/gradcam_examples.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"\\nüî• Grad-CAM visualization saved!\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Grad-CAM not available. Install with: pip install grad-cam\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Grad-CAM visualization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üíæ Export & Deploy\n",
    "\n",
    "Save model and metrics, upload to GCS for automated deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "results = {\n",
    "    \"config\": CONFIG,\n",
    "    \"test_metrics\": {k: float(v) for k, v in test_metrics.items()},\n",
    "    \"best_epoch\": int(checkpoint[\"epoch\"]) + 1,\n",
    "    \"total_params\": model.get_num_total_params(),\n",
    "    \"trainable_params\": model.get_num_trainable_params(),\n",
    "    \"history\": {k: [float(x) for x in v] for k, v in history.items()},\n",
    "    \"roc_auc\": float(roc_auc),\n",
    "}\n",
    "\n",
    "with open(f\"{CONFIG['output_dir']}/results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "model_path = f\"{CONFIG['output_dir']}/{CONFIG['model_filename']}\"\n",
    "model_size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nüíæ Artifacts saved:\")\n",
    "print(f\"   Model: {model_path} ({model_size_mb:.1f} MB)\")\n",
    "print(f\"   Results: {CONFIG['output_dir']}/results.json\")\n",
    "print(f\"   Training curves: {CONFIG['output_dir']}/training_curves.png\")\n",
    "print(f\"   Evaluation: {CONFIG['output_dir']}/evaluation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to GCS\n",
    "UPLOAD_TO_GCS = True  # Set to False to skip upload\n",
    "\n",
    "if UPLOAD_TO_GCS:\n",
    "    print(f\"\\n‚òÅÔ∏è Uploading to gs://{GCS_BUCKET}/...\")\n",
    "    \n",
    "    client = storage.Client(project=GCP_PROJECT)\n",
    "    bucket = client.bucket(GCS_BUCKET)\n",
    "    \n",
    "    # Upload model\n",
    "    blob = bucket.blob(GCS_MODEL_PATH)\n",
    "    blob.upload_from_filename(model_path)\n",
    "    print(f\"   ‚úÖ Model ‚Üí gs://{GCS_BUCKET}/{GCS_MODEL_PATH}\")\n",
    "    \n",
    "    # Upload results\n",
    "    results_gcs_path = \"training/results.json\"\n",
    "    blob = bucket.blob(results_gcs_path)\n",
    "    blob.upload_from_filename(f\"{CONFIG['output_dir']}/results.json\")\n",
    "    print(f\"   ‚úÖ Results ‚Üí gs://{GCS_BUCKET}/{results_gcs_path}\")\n",
    "    \n",
    "    # Upload visualizations\n",
    "    for viz_file in [\"training_curves.png\", \"evaluation.png\", \"gradcam_examples.png\", \"sample_images.png\"]:\n",
    "        local_path = f\"{CONFIG['output_dir']}/{viz_file}\"\n",
    "        if os.path.exists(local_path):\n",
    "            gcs_path = f\"training/{viz_file}\"\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f\"   ‚úÖ {viz_file} ‚Üí gs://{GCS_BUCKET}/{gcs_path}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Upload complete! Model ready for deployment.\")\n",
    "    print(f\"   The CD pipeline will automatically deploy this model.\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è GCS upload skipped. To deploy manually:\")\n",
    "    print(f\"   1. Download {CONFIG['model_filename']} from this notebook\")\n",
    "    print(f\"   2. Place it at models/checkpoints/best_model.pt in the repo\")\n",
    "    print(f\"   3. Push to main ‚Äî CD will build and deploy automatically\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model file (for manual deployment)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"üì• Downloading model checkpoint...\")\n",
    "    files.download(model_path)\n",
    "    print(\"‚úÖ Download started!\")\n",
    "except ImportError:\n",
    "    print(f\"‚ÑπÔ∏è Not running in Colab. Model is at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üìù Summary\n",
    "\n",
    "Training complete! Here's what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Model: {CONFIG['model_name']}\")\n",
    "print(f\"   Parameters: {model.get_num_total_params():,}\")\n",
    "print(f\"   Size: {model_size_mb:.1f} MB\")\n",
    "\n",
    "print(f\"\\nüèãÔ∏è Training:\")\n",
    "print(f\"   Epochs: {checkpoint['epoch'] + 1} / {CONFIG['epochs']}\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Test Results:\")\n",
    "print(f\"   Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"   F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"   AUC-ROC:   {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n‚òÅÔ∏è GCS:\")\n",
    "print(f\"   Bucket: gs://{GCS_BUCKET}\")\n",
    "print(f\"   Model: gs://{GCS_BUCKET}/{GCS_MODEL_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Done! Model is ready for deployment.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
