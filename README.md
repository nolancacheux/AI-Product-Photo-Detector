# AI Product Photo Detector

[![Python 3.11+](https://img.shields.io/badge/Python-3.11%2B-3776AB?logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C?logo=pytorch&logoColor=white)](https://pytorch.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-009688?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![GCP Cloud Run](https://img.shields.io/badge/GCP-Cloud%20Run-4285F4?logo=googlecloud&logoColor=white)](https://cloud.google.com/run)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?logo=docker&logoColor=white)](https://docker.com)
[![DVC](https://img.shields.io/badge/DVC-Pipeline-945DD6?logo=dvc&logoColor=white)](https://dvc.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![CI](https://img.shields.io/github/actions/workflow/status/nolancacheux/AI-Product-Photo-Detector/ci.yml?label=CI&logo=githubactions&logoColor=white)](https://github.com/nolancacheux/AI-Product-Photo-Detector/actions)

**Production-grade MLOps pipeline for detecting AI-generated product photos in e-commerce listings.**

A complete end-to-end machine learning system â€” from data ingestion and model training to API serving, monitoring, and cloud deployment â€” built with modern MLOps best practices.

> **Live API** â†’ [ai-product-detector-714127049161.europe-west1.run.app](https://ai-product-detector-714127049161.europe-west1.run.app)
> &nbsp;|&nbsp; **Swagger UI** â†’ [/docs](https://ai-product-detector-714127049161.europe-west1.run.app/docs)

---

## Features

- ğŸ” **Binary image classification** â€” Detects whether a product photo is real or AI-generated
- ğŸ§  **EfficientNet-B0 backbone** â€” Transfer learning with pretrained ImageNet weights via `timm`
- ğŸ”¥ **Grad-CAM explainability** â€” Visual heatmaps showing which image regions drive the prediction
- âš¡ **FastAPI serving** â€” Async API with single and batch prediction endpoints
- ğŸ³ **Docker-first** â€” Multi-service stack with Compose (API + UI + MLflow + Prometheus + Grafana)
- ğŸ“Š **Full observability** â€” Prometheus metrics, Grafana dashboards, structured JSON logging
- ğŸ”„ **DVC pipelines** â€” Reproducible data download â†’ validation â†’ training workflow
- ğŸš€ **CI/CD to GCP Cloud Run** â€” Automated deploy on push to `main` via GitHub Actions
- ğŸ›¡ï¸ **Production hardening** â€” Rate limiting, API key auth, CORS, input validation, drift detection
- ğŸ§ª **Comprehensive testing** â€” Unit, integration, and load tests (Locust)
- ğŸ¨ **Streamlit UI** â€” Interactive web interface for drag-and-drop image analysis

---

## Architecture

<p align="center">
  <img src="docs/images/architecture.svg" alt="System Architecture" width="800"/>
</p>

The system follows a modular architecture with clear separation between training, serving, and monitoring concerns. See [`docs/ARCHITECTURE.md`](docs/ARCHITECTURE.md) for the full breakdown.

---

## Tech Stack

| Layer | Technologies |
|---|---|
| **Deep Learning** | PyTorch, torchvision, timm (EfficientNet-B0), Grad-CAM |
| **API** | FastAPI, Uvicorn, Pydantic v2, slowapi (rate limiting) |
| **MLOps** | DVC (pipelines + data versioning), MLflow (experiment tracking) |
| **Monitoring** | Prometheus, Grafana, structlog (JSON), custom drift detection |
| **Infrastructure** | Docker, Docker Compose, GCP Cloud Run, Artifact Registry |
| **CI/CD** | GitHub Actions (lint â†’ type-check â†’ test â†’ security â†’ deploy) |
| **Quality** | Ruff (lint + format), mypy (strict), pytest + coverage, Locust (load testing) |
| **UI** | Streamlit |

---

## Quick Start

### Prerequisites

- Python 3.11+
- [uv](https://docs.astral.sh/uv/) (recommended) or pip
- Docker & Docker Compose (for full stack)

### Installation

```bash
# Clone the repository
git clone https://github.com/nolancacheux/AI-Product-Photo-Detector.git
cd AI-Product-Photo-Detector

# Install dependencies
make install        # production only
make dev            # development (includes linting, testing, pre-commit)
```

### Train a Model

```bash
# Download the CIFAKE dataset (2500 images per class)
make data

# Train with default config
make train

# Or reproduce the full DVC pipeline (download â†’ validate â†’ train)
make dvc-repro
```

Training configuration is in [`configs/train_config.yaml`](configs/train_config.yaml). Key hyperparameters:

| Parameter | Value |
|---|---|
| Architecture | EfficientNet-B0 (pretrained) |
| Image size | 224Ã—224 |
| Batch size | 64 |
| Epochs | 15 |
| Learning rate | 0.001 |
| Scheduler | Cosine annealing with warmup |
| Early stopping | Patience: 5 epochs |

### Serve the API

```bash
# Local development (with hot reload)
make serve

# Production (Docker)
make docker-build
make docker-run

# Full stack (API + UI + MLflow + Prometheus + Grafana)
make docker-up
```

### Run Tests

```bash
make test           # Unit + integration tests with coverage
make lint           # Ruff + mypy
make load-test      # Locust load test (10 users, 60s)
```

---

## API Documentation

Base URL: `https://ai-product-detector-714127049161.europe-west1.run.app`

Interactive documentation: [`/docs`](https://ai-product-detector-714127049161.europe-west1.run.app/docs) (Swagger UI)

### Endpoints

#### `POST /predict` â€” Single Image Prediction

Classifies an image as `real` or `ai_generated` with a confidence score.

```bash
curl -X POST https://ai-product-detector-714127049161.europe-west1.run.app/predict \
  -H "X-API-Key: YOUR_API_KEY" \
  -F "file=@product_photo.jpg"
```

**Response:**
```json
{
  "prediction": "ai_generated",
  "probability": 0.87,
  "confidence": "high",
  "inference_time_ms": 45.2,
  "model_version": "1.0.0"
}
```

| Field | Type | Description |
|---|---|---|
| `prediction` | string | `real` or `ai_generated` |
| `probability` | float | Probability of being AI-generated (0.0â€“1.0) |
| `confidence` | string | `low` (<0.3), `medium` (0.3â€“0.8), `high` (>0.8) |
| `inference_time_ms` | float | Inference latency in milliseconds |
| `model_version` | string | Model version used |

**Constraints:** JPEG, PNG, or WebP â€” max 5 MB â€” rate limited to 30 req/min.

---

#### `POST /predict/batch` â€” Batch Prediction

Classify up to 10 images in a single request.

```bash
curl -X POST https://ai-product-detector-714127049161.europe-west1.run.app/predict/batch \
  -H "X-API-Key: YOUR_API_KEY" \
  -F "files=@photo1.jpg" \
  -F "files=@photo2.png"
```

**Response:**
```json
{
  "results": [
    {
      "filename": "photo1.jpg",
      "prediction": "ai_generated",
      "probability": 0.87,
      "confidence": "high",
      "error": null
    },
    {
      "filename": "photo2.png",
      "prediction": "real",
      "probability": 0.12,
      "confidence": "high",
      "error": null
    }
  ],
  "total": 2,
  "successful": 2,
  "failed": 0,
  "total_inference_time_ms": 89.5,
  "model_version": "1.0.0"
}
```

**Constraints:** Max 10 images â€” 5 MB each â€” 50 MB total payload â€” rate limited to 5 req/min.

---

#### `POST /predict/explain` â€” Prediction with Grad-CAM Heatmap

Returns the prediction plus a base64-encoded JPEG heatmap showing which regions influenced the decision.

```bash
curl -X POST https://ai-product-detector-714127049161.europe-west1.run.app/predict/explain \
  -H "X-API-Key: YOUR_API_KEY" \
  -F "file=@product_photo.jpg"
```

**Response:**
```json
{
  "prediction": "ai_generated",
  "probability": 0.87,
  "confidence": "high",
  "heatmap_base64": "/9j/4AAQ...",
  "inference_time_ms": 120.5,
  "model_version": "1.0.0"
}
```

**Constraints:** Rate limited to 10 req/min (heavier computation).

---

#### `GET /health` â€” Readiness Probe

```bash
curl https://ai-product-detector-714127049161.europe-west1.run.app/health
```

```json
{
  "status": "healthy",
  "model_loaded": true,
  "model_version": "1.0.0",
  "uptime_seconds": 3600.5,
  "active_requests": 2,
  "drift_detected": false,
  "predictions_total": 1542
}
```

---

#### `GET /healthz` â€” Liveness Probe

Lightweight probe for Kubernetes / Cloud Run. Returns `200` if the process is alive.

```bash
curl https://ai-product-detector-714127049161.europe-west1.run.app/healthz
```

---

#### `GET /metrics` â€” Prometheus Metrics

Exposes all application metrics in Prometheus text format.

```bash
curl https://ai-product-detector-714127049161.europe-west1.run.app/metrics
```

---

#### `GET /drift` â€” Drift Detection Status

Returns current drift monitoring metrics including alerts.

```bash
curl https://ai-product-detector-714127049161.europe-west1.run.app/drift
```

---

#### `GET /privacy` â€” Privacy Policy

Returns data handling and privacy information (GDPR-friendly â€” no data is stored).

---

### Authentication

Authentication is controlled via environment variables:

| Variable | Description |
|---|---|
| `API_KEYS` | Comma-separated list of valid API keys |
| `REQUIRE_AUTH` | Set to `true` to enforce authentication (rejects all requests if no keys configured) |

When auth is disabled (default for local dev), all endpoints are publicly accessible.

### Error Handling

All errors follow a consistent format:

```json
{
  "error": "Invalid image format",
  "detail": "Supported formats: JPEG, PNG, WebP. Got: image/gif"
}
```

| Status | Meaning |
|---|---|
| `400` | Invalid input (bad format, empty batch) |
| `401` | Missing or invalid API key |
| `413` | File too large (>5 MB or batch >50 MB) |
| `429` | Rate limit exceeded |
| `503` | Model not loaded / service unavailable |

---

## MLOps Pipeline

### DVC â€” Reproducible Pipelines

The entire workflow is orchestrated with [DVC](https://dvc.org):

```yaml
# dvc.yaml
stages:
  download:   # Download CIFAKE dataset
  validate:   # Validate data integrity â†’ reports/data_validation.json
  train:      # Train model â†’ models/checkpoints/best_model.pt
```

```bash
dvc repro           # Run the full pipeline
dvc repro train     # Re-run training only
dvc status          # Check what's changed
```

### CI/CD â€” GitHub Actions

Three workflows automate quality and deployment:

| Workflow | Trigger | Pipeline |
|---|---|---|
| **CI** ([`ci.yml`](.github/workflows/ci.yml)) | Push / PR to `main` | Lint â†’ Type check â†’ Test (3.11 + 3.12) â†’ Security scan â†’ Docker build â†’ Deploy |
| **Deploy** ([`deploy.yml`](.github/workflows/deploy.yml)) | Manual dispatch | Build â†’ Push to Artifact Registry â†’ Deploy to Cloud Run â†’ Health check |
| **DVC** ([`dvc.yml`](.github/workflows/dvc.yml)) | Manual dispatch | Pull data â†’ Reproduce pipeline â†’ Upload model artifact |

The CI pipeline automatically deploys to GCP Cloud Run on every push to `main` after all checks pass.

### Experiment Tracking â€” MLflow

All training runs are logged to MLflow with hyperparameters, metrics, and model artifacts:

```bash
make mlflow         # Start MLflow UI on port 5000
```

---

## Cloud Deployment

### GCP Cloud Run

The API is deployed as a serverless container on Google Cloud Run:

```
Region:             europe-west1
Memory:             1 Gi (configurable: 512Mi / 1Gi / 2Gi)
Port:               8080
Container Registry: europe-west1-docker.pkg.dev
Scaling:            0 â†’ N (automatic)
Auth:               API key via X-API-Key header
```

**Deployment flow:**

```
git push main â†’ CI passes â†’ Docker build â†’ Push to Artifact Registry â†’ Deploy to Cloud Run â†’ Health check
```

**Manual deploy / rollback:**

```bash
# Deploy latest
make deploy

# Rollback to specific commit
gh workflow run deploy.yml -f image_tag=<commit-sha>

# Dry run (validate only)
gh workflow run deploy.yml -f dry_run=true
```

### Docker

```bash
# Build the API image (CPU-optimized PyTorch)
docker build -f docker/Dockerfile -t ai-product-detector:1.0.0 .

# Run standalone
docker run --rm -p 8080:8080 -v ./models:/app/models:ro ai-product-detector:1.0.0

# Full stack with docker-compose
docker compose up -d    # API + Streamlit UI + MLflow + Prometheus + Grafana
docker compose logs -f  # Follow logs
docker compose down     # Tear down
```

**Service ports (docker-compose):**

| Service | Port | URL |
|---|---|---|
| API | 8080 | http://localhost:8080 |
| Streamlit UI | 8501 | http://localhost:8501 |
| MLflow | 5000 | http://localhost:5000 |
| Prometheus | 9090 | http://localhost:9090 |
| Grafana | 3000 | http://localhost:3000 |

---

## Monitoring

### Prometheus Metrics

The API exposes a comprehensive set of custom metrics at `/metrics`:

| Metric | Type | Description |
|---|---|---|
| `aidetect_predictions_total` | Counter | Total predictions by status/class/confidence |
| `aidetect_prediction_latency_seconds` | Histogram | Per-prediction latency distribution |
| `aidetect_prediction_probability` | Histogram | Distribution of prediction probabilities |
| `aidetect_batch_predictions_total` | Counter | Batch request count |
| `aidetect_batch_size` | Histogram | Number of images per batch request |
| `aidetect_batch_latency_seconds` | Histogram | Batch processing time |
| `aidetect_image_validation_errors_total` | Counter | Validation errors by type |
| `aidetect_model_loaded` | Gauge | Model load status (0/1) |
| `aidetect_request_size_bytes` | Histogram | Request payload size |
| `aidetect_response_size_bytes` | Histogram | Response payload size |
| `http_request_duration_seconds` | Histogram | HTTP request latency by endpoint |
| `http_requests_total` | Counter | HTTP requests by method/endpoint/status |

### Drift Detection

A custom drift detector monitors prediction distribution in real-time:

- Sliding window over the last 1000 predictions
- Tracks mean probability, confidence distribution, and class ratios
- Alerts when metrics deviate from baseline (configurable threshold)
- Status available at `GET /drift`

### Grafana Dashboards

Pre-configured Grafana dashboards with Prometheus as data source. Auto-provisioned via `configs/grafana/provisioning/`.

Default credentials: `admin` / `admin`

### Structured Logging

All application logs use `structlog` with JSON output, including:

- Request ID tracking (via `X-Request-ID` header)
- Cloud Trace context correlation (GCP)
- Prediction metadata (result, probability, latency)

---

## Project Structure

```
AI-Product-Photo-Detector/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                  # CI: lint + test + security + deploy
â”‚       â”œâ”€â”€ deploy.yml              # Manual deploy / rollback
â”‚       â””â”€â”€ dvc.yml                 # DVC pipeline reproduction
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ grafana/provisioning/       # Grafana dashboard + datasource configs
â”‚   â”œâ”€â”€ inference_config.yaml       # API server configuration
â”‚   â”œâ”€â”€ prometheus.yml              # Prometheus scrape targets
â”‚   â””â”€â”€ train_config.yaml          # Training hyperparameters
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile                  # Production image (CPU PyTorch, non-root)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ images/architecture.svg     # System architecture diagram
â”‚   â”œâ”€â”€ ARCHITECTURE.md             # Detailed architecture documentation
â”‚   â”œâ”€â”€ CONTRIBUTING.md             # Contribution guidelines
â”‚   â”œâ”€â”€ COSTS.md                    # Cloud cost analysis
â”‚   â””â”€â”€ PRD.md                      # Product requirements document
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_sample_data.py       # Generate sample test images
â”‚   â”œâ”€â”€ download_cifake.py          # Download CIFAKE dataset
â”‚   â””â”€â”€ download_dataset.py         # Generic dataset downloader
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ validate.py             # Dataset validation and integrity checks
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ api.py                  # FastAPI application and routes
â”‚   â”‚   â”œâ”€â”€ auth.py                 # API key authentication (HMAC + constant-time)
â”‚   â”‚   â”œâ”€â”€ explainer.py            # Grad-CAM heatmap generation
â”‚   â”‚   â”œâ”€â”€ predictor.py            # Model inference engine
â”‚   â”‚   â”œâ”€â”€ schemas.py              # Pydantic request/response schemas
â”‚   â”‚   â”œâ”€â”€ shadow.py               # Shadow model comparison
â”‚   â”‚   â””â”€â”€ validation.py           # Image validation utilities
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ drift.py                # Real-time drift detection
â”‚   â”‚   â””â”€â”€ metrics.py              # Prometheus metric definitions
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ augmentation.py         # Data augmentation transforms
â”‚   â”‚   â”œâ”€â”€ dataset.py              # PyTorch Dataset implementation
â”‚   â”‚   â”œâ”€â”€ model.py                # EfficientNet-B0 architecture
â”‚   â”‚   â””â”€â”€ train.py                # Training loop with MLflow tracking
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â””â”€â”€ app.py                  # Streamlit web interface
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py               # Settings management (Pydantic Settings)
â”‚       â””â”€â”€ logger.py               # Structured logging setup
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â””â”€â”€ locustfile.py           # Load testing scenarios
â”‚   â”œâ”€â”€ test_api.py                 # API endpoint tests
â”‚   â”œâ”€â”€ test_auth.py                # Authentication tests
â”‚   â”œâ”€â”€ test_batch.py               # Batch prediction tests
â”‚   â”œâ”€â”€ test_drift.py               # Drift detection tests
â”‚   â”œâ”€â”€ test_explainer.py           # Grad-CAM tests
â”‚   â”œâ”€â”€ test_model.py               # Model architecture tests
â”‚   â”œâ”€â”€ test_predictor.py           # Inference engine tests
â”‚   â””â”€â”€ ...                         # + config, dataset, metrics, validation tests
â”œâ”€â”€ docker-compose.yml              # Full stack orchestration
â”œâ”€â”€ dvc.yaml                        # DVC pipeline definition
â”œâ”€â”€ Makefile                        # Development commands
â”œâ”€â”€ pyproject.toml                  # Project metadata and dependencies
â””â”€â”€ .pre-commit-config.yaml         # Pre-commit hooks (ruff)
```

---

## Contributing

Contributions are welcome. Please read [`docs/CONTRIBUTING.md`](docs/CONTRIBUTING.md) for guidelines.

```bash
# Setup development environment
make dev

# Run quality checks before submitting
make lint
make test
```

This project uses:
- **Ruff** for linting and formatting
- **mypy** (strict mode) for type checking
- **pre-commit** hooks for automated checks
- **Conventional commits** (`feat:`, `fix:`, `docs:`, etc.)

---

## License

This project is licensed under the MIT License â€” see [LICENSE](LICENSE) for details.

---

**Built by [Nolan Cacheux](https://github.com/nolancacheux)**
