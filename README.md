<p align="center">
  <h1 align="center">ğŸ” AI Product Photo Detector</h1>
  <p align="center">
    <strong>Production-grade MLOps system for detecting AI-generated product photos in e-commerce</strong>
  </p>
</p>

<p align="center">
  <a href="https://github.com/nolancacheux/AI-Product-Photo-Detector/actions/workflows/ci.yml"><img src="https://github.com/nolancacheux/AI-Product-Photo-Detector/actions/workflows/ci.yml/badge.svg?branch=main" alt="CI"></a>
  <a href="https://github.com/nolancacheux/AI-Product-Photo-Detector/actions/workflows/cd.yml"><img src="https://github.com/nolancacheux/AI-Product-Photo-Detector/actions/workflows/cd.yml/badge.svg" alt="CD"></a>
  <a href="https://github.com/nolancacheux/AI-Product-Photo-Detector/actions/workflows/model-training.yml"><img src="https://github.com/nolancacheux/AI-Product-Photo-Detector/actions/workflows/model-training.yml/badge.svg" alt="Training"></a>
  <br/>
  <a href="https://www.python.org/"><img src="https://img.shields.io/badge/Python-3.11%20%7C%203.12-3776AB?logo=python&logoColor=white" alt="Python"></a>
  <a href="https://pytorch.org/"><img src="https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?logo=pytorch&logoColor=white" alt="PyTorch"></a>
  <a href="https://fastapi.tiangolo.com/"><img src="https://img.shields.io/badge/FastAPI-0.100+-009688?logo=fastapi&logoColor=white" alt="FastAPI"></a>
  <a href="https://cloud.google.com/run"><img src="https://img.shields.io/badge/Cloud%20Run-Deployed-4285F4?logo=googlecloud&logoColor=white" alt="Cloud Run"></a>
  <a href="https://docker.com/"><img src="https://img.shields.io/badge/Docker-Ready-2496ED?logo=docker&logoColor=white" alt="Docker"></a>
  <a href="https://dvc.org/"><img src="https://img.shields.io/badge/DVC-Pipeline-945DD6?logo=dvc&logoColor=white" alt="DVC"></a>
  <a href="https://github.com/nolancacheux/AI-Product-Photo-Detector/blob/main/LICENSE"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License"></a>
  <a href="https://colab.research.google.com/github/nolancacheux/AI-Product-Photo-Detector/blob/main/notebooks/train_colab.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
</p>

<p align="center">
  End-to-end machine learning system â€” from data ingestion and GPU training on Vertex AI<br/>
  to API serving, real-time monitoring, and automated cloud deployment.
</p>

---

## ğŸŒ Live Demo

| Resource | URL |
|----------|-----|
| **ğŸš€ REST API** | [`ai-product-detector-714127049161.europe-west1.run.app`](https://ai-product-detector-714127049161.europe-west1.run.app) |
| **ğŸ–¥ï¸ Web UI** | [`ai-product-detector-ui-714127049161.europe-west1.run.app`](https://ai-product-detector-ui-714127049161.europe-west1.run.app) |
| **ğŸ“– Swagger Docs** | [`/docs`](https://ai-product-detector-714127049161.europe-west1.run.app/docs) |
| **ğŸ“Š Health Check** | [`/health`](https://ai-product-detector-714127049161.europe-west1.run.app/health) |
| **ğŸ“ˆ Metrics** | [`/metrics`](https://ai-product-detector-714127049161.europe-west1.run.app/metrics) |

```bash
# Try it now â€” single prediction
curl -X POST https://ai-product-detector-714127049161.europe-west1.run.app/predict \
  -H "X-API-Key: YOUR_API_KEY" \
  -F "file=@product_photo.jpg"
```

---

## ğŸ—ï¸ Architecture

### System Overview

```mermaid
graph TB
    subgraph Clients["Client Layer"]
        CLI[cURL / HTTPie]
        UI[Streamlit UI]
        SDK[Python SDK]
    end

    subgraph CloudRun["Google Cloud Run"]
        API[FastAPI Server]
        AUTH[Auth Middleware]
        RL[Rate Limiter]
        PRED[Predictor Engine]
        EXPL[Grad-CAM Explainer]
        DRIFT[Drift Detector]
        METRICS[Prometheus Metrics]
    end

    subgraph Storage["Google Cloud Storage"]
        GCS_DATA[Training Data]
        GCS_MODELS[Model Checkpoints]
        GCS_STAGING[Vertex AI Staging]
    end

    subgraph Registry["Artifact Registry"]
        IMG_API[API Image]
        IMG_TRAIN[Training Image]
        IMG_UI[UI Image]
    end

    subgraph Training["Vertex AI"]
        VERTEX[Custom Training Job]
        GPU["n1-standard-4 + T4 GPU"]
    end

    subgraph Monitoring["Observability"]
        PROM[Prometheus]
        GRAF[Grafana Dashboards]
        LOGS[Structured Logging]
    end

    CLI --> API
    UI --> API
    SDK --> API
    API --> AUTH --> RL --> PRED
    API --> EXPL
    API --> DRIFT
    API --> METRICS
    PRED --> GCS_MODELS
    VERTEX --> GPU
    GPU --> GCS_MODELS
    GCS_DATA --> VERTEX
    IMG_TRAIN --> VERTEX
    IMG_API --> CloudRun
    METRICS --> PROM --> GRAF
    API --> LOGS

    style CloudRun fill:#e8f5e9,stroke:#2e7d32
    style Training fill:#e3f2fd,stroke:#1565c0
    style Storage fill:#fff3e0,stroke:#ef6c00
    style Monitoring fill:#fce4ec,stroke:#c62828
```

### CI/CD Pipeline

```mermaid
graph LR
    subgraph CI["CI Pipeline"]
        PUSH[Git Push] --> LINT[Ruff Lint]
        PUSH --> TYPE[mypy Type Check]
        PUSH --> TEST["pytest\n3.11 + 3.12"]
        PUSH --> SEC[Security Scan]
        LINT --> DBUILD[Docker Build Test]
        TEST --> DBUILD
    end

    subgraph CD["CD Pipeline"]
        DBUILD --> WAIT[Wait for CI]
        WAIT --> BUILD[Build Image]
        BUILD --> PUSH_REG[Push to\nArtifact Registry]
        PUSH_REG --> DEPLOY[Deploy to\nCloud Run]
        DEPLOY --> SMOKE[Smoke Test]
    end

    subgraph ModelCD["Model Training Pipeline"]
        TRIGGER[Manual Dispatch] --> DATA[Verify GCS Data]
        DATA --> TBUILD[Build Training Image]
        TBUILD --> VTRAIN["Vertex AI\nGPU Training"]
        VTRAIN --> EVAL[Evaluate Model]
        EVAL --> GATE{"Quality Gate\nacc â‰¥ 0.85\nF1 â‰¥ 0.80"}
        GATE -->|Pass| MDEPLOY[Deploy New Model]
        GATE -->|Fail| REJECT[Block Deploy]
    end

    style CI fill:#e8f5e9,stroke:#2e7d32
    style CD fill:#e3f2fd,stroke:#1565c0
    style ModelCD fill:#fff3e0,stroke:#ef6c00
```

### ML Pipeline

```mermaid
graph LR
    subgraph Data["Data Stage"]
        HF[HuggingFace\nDatasets] --> DL[Download]
        CIFAKE[CIFAKE\nDataset] --> DL
        DL --> VAL[Validate\nIntegrity]
        VAL --> SPLIT["Train / Val / Test\nSplit"]
    end

    subgraph Train["Training Stage"]
        SPLIT --> AUG[Augmentation\nFlip, Rotate, Jitter]
        AUG --> MODEL["EfficientNet-B0\nTransfer Learning"]
        MODEL --> OPT["AdamW + Cosine\nAnnealing"]
        OPT --> CKPT[Checkpoint\nbest_model.pt]
    end

    subgraph Evaluate["Evaluation Stage"]
        CKPT --> METRICS_E[Accuracy, F1\nPrecision, Recall]
        METRICS_E --> GATE_E{"Quality Gate"}
        GATE_E -->|Pass| REG[Model Registry\nGCS + MLflow]
        GATE_E -->|Fail| RETRAIN[Retrain]
    end

    subgraph Deploy["Deploy Stage"]
        REG --> DOCKER[Docker Build]
        DOCKER --> CR[Cloud Run Deploy]
        CR --> MONITOR[Monitor\nDrift Detection]
        MONITOR -->|Drift| RETRAIN
    end

    style Data fill:#f3e5f5,stroke:#7b1fa2
    style Train fill:#e8f5e9,stroke:#2e7d32
    style Evaluate fill:#e3f2fd,stroke:#1565c0
    style Deploy fill:#fff3e0,stroke:#ef6c00
```

---

## âœ¨ Features

### ğŸ§  Core ML
- **Binary image classification** â€” Real vs AI-generated product photos
- **EfficientNet-B0 backbone** â€” Transfer learning with pretrained ImageNet weights via `timm`
- **Grad-CAM explainability** â€” Visual heatmaps showing which regions drive the prediction
- **Data augmentation** â€” Horizontal flip, rotation, color jitter, random crop
- **Cosine annealing** â€” Learning rate scheduling with warmup

### ğŸš€ API & Serving
- **FastAPI async server** â€” Single, batch (up to 10), and explainability endpoints
- **API key authentication** â€” HMAC-based constant-time comparison
- **Rate limiting** â€” Per-endpoint configurable limits via `slowapi`
- **Input validation** â€” File type, size, and format verification
- **Structured responses** â€” Pydantic v2 schemas with confidence levels

### ğŸ”„ MLOps
- **DVC pipelines** â€” Reproducible `download â†’ validate â†’ train` workflow
- **MLflow experiment tracking** â€” Hyperparameters, metrics, and model artifacts
- **Vertex AI training** â€” Automated GPU training with T4 on GCP
- **Quality gate** â€” Automated accuracy/F1 thresholds before deployment
- **Model versioning** â€” GCS-backed model registry with DVC tracking

### ğŸ“Š Monitoring & Observability
- **Prometheus metrics** â€” 12+ custom metrics (latency, throughput, probability distribution)
- **Grafana dashboards** â€” Pre-configured, auto-provisioned dashboards
- **Drift detection** â€” Real-time prediction distribution monitoring (sliding window)
- **Structured logging** â€” JSON output via `structlog` with request ID correlation

### ğŸ” Security
- **API key auth** â€” Optional, enforced in production via environment variables
- **Rate limiting** â€” Abuse prevention on all prediction endpoints
- **Non-root containers** â€” Docker images run as unprivileged users
- **Security scanning** â€” `pip-audit` + `bandit` in CI pipeline
- **CORS configuration** â€” Configurable allowed origins

### ğŸ—ï¸ Infrastructure
- **Terraform IaC** â€” GCS, Artifact Registry, Cloud Run, IAM, budget alerts
- **Docker Compose** â€” Full local stack (API + UI + MLflow + Prometheus + Grafana)
- **GitHub Actions CI/CD** â€” Automated lint, test, build, deploy on every push
- **Serverless scaling** â€” Cloud Run auto-scales 0â†’N based on traffic

---

## ğŸš€ Quick Start

### Local Development

**Prerequisites:** Python 3.11+, [uv](https://docs.astral.sh/uv/) (recommended) or pip, Docker & Docker Compose

```bash
# Clone the repository
git clone https://github.com/nolancacheux/AI-Product-Photo-Detector.git
cd AI-Product-Photo-Detector

# Install all dependencies (dev + ui + pre-commit hooks)
make dev

# Download dataset and train a model
make data           # Download CIFAKE (2500 images/class)
make train          # Train EfficientNet-B0

# Start the API server with hot reload
make serve          # â†’ http://localhost:8000

# Or start the full stack with Docker Compose
make docker-up      # API + UI + MLflow + Prometheus + Grafana
```

**Service URLs (Docker Compose):**

| Service | URL | Description |
|---------|-----|-------------|
| **API** | http://localhost:8080 | FastAPI inference server |
| **Streamlit UI** | http://localhost:8501 | Drag-and-drop image analysis |
| **MLflow** | http://localhost:5000 | Experiment tracking UI |
| **Prometheus** | http://localhost:9090 | Metrics collection |
| **Grafana** | http://localhost:3000 | Monitoring dashboards |

### Production Deployment

```bash
# 1. Provision infrastructure with Terraform
cd terraform
cp terraform.tfvars.example terraform.tfvars  # Edit with your GCP project
terraform init && terraform apply

# 2. Push to main â€” CI/CD handles the rest
git push origin main
# CI: lint â†’ type-check â†’ test (3.11 + 3.12) â†’ security scan
# CD: build image â†’ push to Artifact Registry â†’ deploy to Cloud Run â†’ smoke test

# 3. Trigger GPU training on Vertex AI
gh workflow run model-training.yml \
  -f epochs=15 \
  -f batch_size=64 \
  -f auto_deploy=true
```

---

## ğŸ“¡ API Documentation

**Base URL:** `https://ai-product-detector-714127049161.europe-west1.run.app`
&nbsp;|&nbsp; **Interactive docs:** [`/docs`](https://ai-product-detector-714127049161.europe-west1.run.app/docs)

### Endpoints

| Method | Endpoint | Description | Rate Limit |
|--------|----------|-------------|------------|
| `POST` | `/predict` | Single image classification | 30/min |
| `POST` | `/predict/batch` | Batch classification (up to 10) | 5/min |
| `POST` | `/predict/explain` | Prediction + Grad-CAM heatmap | 10/min |
| `GET` | `/health` | Readiness probe (model status, uptime, drift) | â€” |
| `GET` | `/healthz` | Lightweight liveness probe | â€” |
| `GET` | `/metrics` | Prometheus metrics (text format) | â€” |
| `GET` | `/drift` | Drift detection status | â€” |
| `GET` | `/privacy` | GDPR privacy policy | â€” |

### Single Prediction

```bash
curl -X POST https://ai-product-detector-714127049161.europe-west1.run.app/predict \
  -H "X-API-Key: YOUR_API_KEY" \
  -F "file=@product_photo.jpg"
```

```json
{
  "prediction": "ai_generated",
  "probability": 0.87,
  "confidence": "high",
  "inference_time_ms": 45.2,
  "model_version": "1.0.0"
}
```

### Batch Prediction

```bash
curl -X POST https://ai-product-detector-714127049161.europe-west1.run.app/predict/batch \
  -H "X-API-Key: YOUR_API_KEY" \
  -F "files=@photo1.jpg" \
  -F "files=@photo2.png"
```

```json
{
  "results": [
    { "filename": "photo1.jpg", "prediction": "ai_generated", "probability": 0.87, "confidence": "high" },
    { "filename": "photo2.png", "prediction": "real", "probability": 0.12, "confidence": "high" }
  ],
  "total": 2,
  "successful": 2,
  "failed": 0,
  "total_inference_time_ms": 89.5
}
```

### Explainability (Grad-CAM)

```bash
curl -X POST https://ai-product-detector-714127049161.europe-west1.run.app/predict/explain \
  -H "X-API-Key: YOUR_API_KEY" \
  -F "file=@product_photo.jpg"
```

```json
{
  "prediction": "ai_generated",
  "probability": 0.87,
  "confidence": "high",
  "heatmap_base64": "/9j/4AAQ...",
  "inference_time_ms": 120.5
}
```

### Authentication

| Variable | Description |
|----------|-------------|
| `API_KEYS` | Comma-separated list of valid API keys |
| `REQUIRE_AUTH` | Set to `true` to enforce auth (default: disabled for local dev) |

Pass the key via header: `X-API-Key: YOUR_KEY`

### Error Responses

```json
{ "error": "Invalid image format", "detail": "Supported formats: JPEG, PNG, WebP. Got: image/gif" }
```

| Status | Meaning |
|--------|---------|
| `400` | Invalid input (bad format, empty batch) |
| `401` | Missing or invalid API key |
| `413` | File too large (>5 MB) or batch payload >50 MB |
| `429` | Rate limit exceeded |
| `503` | Model not loaded / service starting |

---

## ğŸ”¬ MLOps Pipeline

### Training Options

| | **Google Colab** | **Vertex AI** | **Local** |
|---|---|---|---|
| **GPU** | Free T4 | T4 on GCP (paid) | CPU or local GPU |
| **Cost** | Free | ~$0.10â€“0.20/run | Free |
| **Time** | ~20 min | ~25 min | ~1â€“2h (CPU) |
| **Dataset** | HuggingFace (high-res) | GCS (auto-uploaded) | CIFAKE (DVC) |
| **Best for** | Quick experiments | Production retraining | Development |

#### Colab (Quick Start)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nolancacheux/AI-Product-Photo-Detector/blob/main/notebooks/train_colab.ipynb)

Open â†’ set runtime to T4 GPU â†’ run all cells â†’ download checkpoint.

#### Vertex AI (Production)

```bash
# Trigger via GitHub Actions
gh workflow run model-training.yml -f epochs=15 -f batch_size=64 -f auto_deploy=true

# Or submit directly
python -m src.training.vertex_submit --epochs 15 --batch-size 64 --sync
```

**Pipeline stages:**

```
[1] Verify Data â†’ [2] Build Image â†’ [3] GPU Training â†’ [4] Evaluate â†’ [5] Quality Gate â†’ [6] Deploy
     (GCS)        (Artifact Reg.)    (Vertex AI T4)      (CPU)        (accâ‰¥0.85,F1â‰¥0.80)  (Cloud Run)
```

#### Local Training

```bash
make data            # Download CIFAKE dataset
make train           # Train with configs/train_config.yaml
make dvc-repro       # Full DVC pipeline: download â†’ validate â†’ train
make mlflow          # Start MLflow UI â†’ http://localhost:5000
```

### DVC Pipeline

```yaml
# dvc.yaml â€” 3-stage reproducible pipeline
stages:
  download:   # Download CIFAKE dataset â†’ data/processed/
  validate:   # Integrity checks â†’ reports/data_validation.json
  train:      # EfficientNet-B0 â†’ models/checkpoints/best_model.pt
```

```bash
dvc repro            # Run full pipeline
dvc repro train      # Re-run training only
dvc status           # Check what changed
```

### Training Configuration

| Parameter | Value |
|-----------|-------|
| Architecture | EfficientNet-B0 (ImageNet pretrained) |
| Image size | 224 Ã— 224 |
| Batch size | 64 |
| Epochs | 15 |
| Optimizer | AdamW, lr=0.001 |
| Scheduler | Cosine annealing with 2-epoch warmup |
| Early stopping | Patience: 5 epochs |
| Augmentation | Flip, rotation, color jitter, random crop |

---

## ğŸ“Š Monitoring & Observability

### Prometheus Metrics

All exposed at `GET /metrics` in Prometheus text format:

| Metric | Type | Description |
|--------|------|-------------|
| `aidetect_predictions_total` | Counter | Total predictions by status / class / confidence |
| `aidetect_prediction_latency_seconds` | Histogram | Per-prediction latency distribution |
| `aidetect_prediction_probability` | Histogram | Probability score distribution |
| `aidetect_batch_predictions_total` | Counter | Batch request count |
| `aidetect_batch_size` | Histogram | Images per batch request |
| `aidetect_batch_latency_seconds` | Histogram | Batch processing time |
| `aidetect_image_validation_errors_total` | Counter | Validation errors by type |
| `aidetect_model_loaded` | Gauge | Model load status (0/1) |
| `aidetect_request_size_bytes` | Histogram | Request payload size |
| `aidetect_response_size_bytes` | Histogram | Response payload size |
| `http_request_duration_seconds` | Histogram | HTTP latency by endpoint |
| `http_requests_total` | Counter | HTTP requests by method / endpoint / status |

### Drift Detection

Real-time monitoring of prediction distribution shifts:

- **Sliding window** over the last 1,000 predictions
- **Tracked signals:** Mean probability, confidence distribution, class ratios
- **Alerting:** Configurable threshold with status at `GET /drift`
- **Feedback loop:** Drift triggers model retraining consideration

### Grafana Dashboards

Pre-configured and auto-provisioned via `configs/grafana/provisioning/`:

- **Request throughput** â€” Requests/sec by endpoint
- **Latency percentiles** â€” p50, p90, p99 per endpoint
- **Prediction distribution** â€” Real vs AI-generated ratio over time
- **Model health** â€” Load status, drift alerts, error rates

Default credentials: `admin` / `admin`

### Structured Logging

```json
{
  "event": "prediction_complete",
  "prediction": "ai_generated",
  "probability": 0.87,
  "latency_ms": 45.2,
  "request_id": "abc-123",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

- `structlog` with JSON output
- Request ID tracking via `X-Request-ID` header
- Cloud Trace context correlation (GCP)

---

## ğŸ› ï¸ Tech Stack

| Category | Technologies |
|----------|-------------|
| **Deep Learning** | PyTorch 2.0+, torchvision, timm (EfficientNet-B0), Grad-CAM |
| **API Framework** | FastAPI, Uvicorn, Pydantic v2, slowapi |
| **Data & MLOps** | DVC (pipelines + versioning), MLflow (experiment tracking), HuggingFace Datasets |
| **Cloud Training** | Vertex AI (CustomContainerTrainingJob), T4 GPU, Google Cloud Storage |
| **Monitoring** | Prometheus, Grafana, structlog (JSON), custom drift detection |
| **Infrastructure** | Docker, Docker Compose, Terraform, GCP Cloud Run, Artifact Registry |
| **CI/CD** | GitHub Actions (3 workflows: CI, CD, Model Training) |
| **Code Quality** | Ruff (lint + format), mypy (strict), pytest + coverage, pre-commit |
| **Load Testing** | Locust, k6 |
| **Security** | pip-audit, bandit, HMAC auth, non-root containers |
| **UI** | Streamlit (deployed on Cloud Run) |

---

## ğŸ“ Project Structure

```
AI-Product-Photo-Detector/
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ ci.yml                          # CI: lint + type-check + test (3.11, 3.12) + security
â”‚   â”œâ”€â”€ cd.yml                          # CD: build â†’ push â†’ deploy Cloud Run â†’ smoke test
â”‚   â””â”€â”€ model-training.yml              # Vertex AI: data â†’ train (GPU) â†’ eval â†’ gate â†’ deploy
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ grafana/                        # Grafana dashboard definitions + provisioning
â”‚   â”œâ”€â”€ inference_config.yaml           # API server configuration
â”‚   â”œâ”€â”€ pipeline_config.yaml            # Vertex AI pipeline parameters
â”‚   â”œâ”€â”€ prometheus.yml                  # Prometheus scrape targets
â”‚   â””â”€â”€ train_config.yaml              # Training hyperparameters
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile                      # Production API image (CPU PyTorch, non-root)
â”‚   â”œâ”€â”€ Dockerfile.training             # Vertex AI GPU training image
â”‚   â”œâ”€â”€ serve.Dockerfile                # Serving-optimized image
â”‚   â”œâ”€â”€ train.Dockerfile                # Local training environment
â”‚   â””â”€â”€ ui.Dockerfile                   # Streamlit UI image
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md                 # System architecture & design decisions
â”‚   â”œâ”€â”€ CICD.md                         # CI/CD pipeline documentation
â”‚   â”œâ”€â”€ CONTRIBUTING.md                 # Contribution guidelines
â”‚   â”œâ”€â”€ COSTS.md                        # Cloud cost analysis
â”‚   â”œâ”€â”€ DEPLOYMENT.md                   # Deployment guide
â”‚   â”œâ”€â”€ INCIDENT_SCENARIO.md            # Incident response playbook
â”‚   â”œâ”€â”€ INFRASTRUCTURE.md               # Infrastructure documentation
â”‚   â”œâ”€â”€ PRD.md                          # Product requirements document
â”‚   â””â”€â”€ TRAINING.md                     # Training pipeline documentation
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ train_colab.ipynb               # Colab notebook â€” free T4 GPU training
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_sample_data.py           # Generate sample test images
â”‚   â”œâ”€â”€ download_cifake.py              # Download CIFAKE dataset
â”‚   â””â”€â”€ download_dataset.py             # Generic dataset downloader
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ validate.py                 # Dataset validation & integrity checks
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ api.py                      # FastAPI application & routes
â”‚   â”‚   â”œâ”€â”€ auth.py                     # API key auth (HMAC, constant-time)
â”‚   â”‚   â”œâ”€â”€ explainer.py                # Grad-CAM heatmap generation
â”‚   â”‚   â”œâ”€â”€ predictor.py                # Model inference engine
â”‚   â”‚   â”œâ”€â”€ rate_limit.py               # Rate limiting configuration
â”‚   â”‚   â”œâ”€â”€ schemas.py                  # Pydantic request/response models
â”‚   â”‚   â”œâ”€â”€ shadow.py                   # Shadow model comparison (A/B testing)
â”‚   â”‚   â”œâ”€â”€ state.py                    # Application state management
â”‚   â”‚   â””â”€â”€ validation.py               # Image validation utilities
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ drift.py                    # Real-time drift detection
â”‚   â”‚   â””â”€â”€ metrics.py                  # Prometheus metric definitions
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ evaluate.py                 # Model evaluation pipeline stage
â”‚   â”‚   â””â”€â”€ training_pipeline.py        # End-to-end training orchestrator
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ augmentation.py             # Data augmentation transforms
â”‚   â”‚   â”œâ”€â”€ dataset.py                  # PyTorch Dataset implementation
â”‚   â”‚   â”œâ”€â”€ gcs.py                      # GCS upload/download helpers
â”‚   â”‚   â”œâ”€â”€ model.py                    # EfficientNet-B0 architecture
â”‚   â”‚   â”œâ”€â”€ train.py                    # Training loop with MLflow tracking
â”‚   â”‚   â””â”€â”€ vertex_submit.py            # Vertex AI job submission CLI
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â””â”€â”€ app.py                      # Streamlit web interface
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py                   # Settings management (Pydantic Settings)
â”‚       â”œâ”€â”€ logger.py                   # Structured logging setup
â”‚       â””â”€â”€ model_loader.py             # Model loading utilities
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf                         # GCS + Artifact Registry + Cloud Run + IAM + Budget
â”‚   â”œâ”€â”€ variables.tf                    # Input variables
â”‚   â”œâ”€â”€ outputs.tf                      # Output values
â”‚   â””â”€â”€ terraform.tfvars.example        # Example configuration
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”œâ”€â”€ locustfile.py               # Locust load testing scenarios
â”‚   â”‚   â””â”€â”€ k6_test.js                  # k6 load testing script
â”‚   â”œâ”€â”€ conftest.py                     # Shared test fixtures
â”‚   â”œâ”€â”€ test_api.py                     # API endpoint tests
â”‚   â”œâ”€â”€ test_augmentation.py            # Augmentation tests
â”‚   â”œâ”€â”€ test_auth.py                    # Authentication tests
â”‚   â”œâ”€â”€ test_batch.py                   # Batch prediction tests
â”‚   â”œâ”€â”€ test_config.py                  # Configuration tests
â”‚   â”œâ”€â”€ test_data_validate.py           # Data validation tests
â”‚   â”œâ”€â”€ test_dataset.py                 # Dataset tests
â”‚   â”œâ”€â”€ test_drift.py                   # Drift detection tests
â”‚   â”œâ”€â”€ test_explainer.py               # Grad-CAM tests
â”‚   â”œâ”€â”€ test_gcs.py                     # GCS helper tests
â”‚   â”œâ”€â”€ test_integration.py             # Integration tests
â”‚   â”œâ”€â”€ test_metrics.py                 # Prometheus metrics tests
â”‚   â”œâ”€â”€ test_model.py                   # Model architecture tests
â”‚   â”œâ”€â”€ test_pipelines.py               # Pipeline orchestration tests
â”‚   â”œâ”€â”€ test_predictor.py               # Inference engine tests
â”‚   â”œâ”€â”€ test_shadow.py                  # Shadow A/B testing tests
â”‚   â”œâ”€â”€ test_train.py                   # Training loop tests
â”‚   â”œâ”€â”€ test_ui.py                      # UI tests
â”‚   â”œâ”€â”€ test_validation.py              # Validation tests
â”‚   â””â”€â”€ test_vertex_submit.py           # Vertex AI submission tests
â”‚
â”œâ”€â”€ docker-compose.yml                  # Full stack: API + UI + MLflow + Prometheus + Grafana
â”œâ”€â”€ dvc.yaml                            # DVC pipeline: download â†’ validate â†’ train
â”œâ”€â”€ Makefile                            # Development commands (make help)
â”œâ”€â”€ pyproject.toml                      # Project metadata, dependencies, tool config
â””â”€â”€ .pre-commit-config.yaml             # Pre-commit hooks (ruff)
```

---

## ğŸ³ Docker

```bash
# Build API image
docker build -f docker/Dockerfile -t ai-product-detector:latest .

# Run standalone
docker run --rm -p 8080:8080 -v ./models:/app/models:ro ai-product-detector:latest

# Full stack (API + UI + MLflow + Prometheus + Grafana)
docker compose up -d
docker compose logs -f
docker compose down
```

---

## â˜ï¸ Cloud Deployment

### Cloud Run Services

| Service | Region | URL |
|---------|--------|-----|
| **API** | europe-west1 | [`ai-product-detector-714127049161.europe-west1.run.app`](https://ai-product-detector-714127049161.europe-west1.run.app) |
| **UI** | europe-west1 | [`ai-product-detector-ui-714127049161.europe-west1.run.app`](https://ai-product-detector-ui-714127049161.europe-west1.run.app) |

**Configuration:** 1 GiB memory, port 8080, auto-scaling 0â†’N, health probes on `/health`

### Terraform Resources

```bash
cd terraform && terraform init && terraform apply
```

Provisions: GCS bucket (versioned), Artifact Registry, Cloud Run service, IAM bindings, budget alerts.

### Deployment Flows

```bash
# Automatic: push to main
git push origin main  # â†’ CI â†’ CD â†’ Cloud Run

# Manual deploy
make deploy  # or: gh workflow run cd.yml

# Rollback
gh workflow run cd.yml -f image_tag=<commit-sha>
```

---

## ğŸ¤ Contributing

Contributions welcome â€” please read [`docs/CONTRIBUTING.md`](docs/CONTRIBUTING.md) first.

```bash
make dev             # Install dev dependencies + pre-commit hooks
make lint            # Ruff + mypy
make test            # pytest with coverage
```

**Conventions:**
- [Conventional commits](https://www.conventionalcommits.org/) â€” `feat:`, `fix:`, `docs:`, `refactor:`, `test:`
- Ruff for linting & formatting
- mypy for type checking
- Pre-commit hooks enforced

---

## ğŸ“„ License

This project is licensed under the MIT License â€” see [LICENSE](LICENSE) for details.

---

<p align="center">
  Built by <a href="https://github.com/nolancacheux">Nolan Cacheux</a>
</p>
