# Incident Scenario: Data Drift Detection and Response

## Executive Summary

This document describes a realistic production incident involving **data drift** in the AI Product Photo Detector system. It covers the full incident lifecycle: detection, analysis, remediation, and prevention measures. This scenario is based on the monitoring infrastructure implemented in `src/monitoring/drift.py`.

---

## 1. Scenario Description

### Context

The AI Product Photo Detector has been running in production for 6 months, serving an e-commerce platform with ~50,000 image classifications per day. The model was trained on a dataset containing AI-generated images from **Stable Diffusion 1.5/2.1, DALL-E 2, and Midjourney v4/v5**.

### Triggering Event

In Q1 2026, a new wave of AI image generators is released:
- **DALL-E 3** (improved photorealism, better text rendering)
- **Midjourney v6** (near-photographic quality, improved lighting)
- **Stable Diffusion XL Turbo** (faster generation with higher fidelity)

These newer models produce images with fundamentally different characteristics:
- Higher resolution and sharper details
- More accurate lighting and shadows
- Fewer typical AI artifacts (distorted hands, blurred text)
- Better color consistency and natural backgrounds

### Impact

| Metric | Before Drift | After Drift | Change |
|--------|-------------|-------------|--------|
| **Accuracy** | 83.2% | 65.1% | -18.1% |
| **Precision (AI class)** | 85.7% | 58.3% | -27.4% |
| **Recall (AI class)** | 80.1% | 71.2% | -8.9% |
| **False Negative Rate** | 19.9% | 41.7% | +21.8% |
| **Low Confidence Ratio** | 12.3% | 34.8% | +22.5% |

The model increasingly **misclassifies newer AI-generated images as real**, allowing fraudulent product listings to pass undetected. An estimated **3,200 fraudulent listings per day** are slipping through the detection system.

---

## 2. Detection

### How the DriftDetector Caught It

The `DriftDetector` class (`src/monitoring/drift.py`) monitors a sliding window of the last 1,000 predictions and compares current statistics against a saved baseline established during initial deployment.

#### Step 1 — Probability Distribution Shift

The detector tracks the **mean prediction probability** across the sliding window. During normal operation, the mean probability hovered around **0.72** (indicating confident predictions leaning toward correct classification). After the drift:

```
Baseline mean probability:     0.72
Current mean probability:      0.54
Probability drift:             0.18  (threshold: 0.15)
```

The `check_drift()` method flagged this as **probability drift exceeded threshold**:

```python
if prob_drift > self.drift_threshold:
    alerts.append(f"Probability drift: {prob_drift:.3f}")
```

#### Step 2 — Increased Low-Confidence Predictions

The detector monitors the ratio of predictions falling within the **low-confidence zone** (probability between 0.2 and 0.8, i.e., within `confidence_threshold=0.3` of the 0.5 decision boundary):

```
Baseline low confidence ratio: 0.123
Current low confidence ratio:  0.348
Confidence drift:              0.225  (threshold: 0.15)
```

This spike indicates the model is increasingly uncertain about its predictions — a clear sign of out-of-distribution data.

#### Step 3 — Prediction Ratio Drift

The class distribution of predictions shifted:

```
Baseline AI prediction ratio:  0.48
Current AI prediction ratio:   0.26
Ratio shift:                   0.22  (threshold: 0.20)
```

The model was classifying far fewer images as AI-generated, consistent with failing to detect new-generation AI images.

#### Alert Timeline

| Day | Event |
|-----|-------|
| Day 0 | DALL-E 3 / Midjourney v6 images start appearing on platform |
| Day 3 | Low confidence ratio rises from 12% to 18% |
| Day 5 | Mean probability drops below 0.65 — first drift alert triggered |
| Day 7 | All three drift indicators exceed thresholds — **DRIFT_DETECTED** |
| Day 8 | On-call engineer investigates the alert |

### Monitoring Endpoint

The `/metrics` endpoint exposed the drift status via `get_status()`:

```json
{
  "window_size": 1000,
  "window_capacity": 1000,
  "mean_probability": 0.5412,
  "low_confidence_ratio": 0.3480,
  "drift_detected": true,
  "drift_score": 0.2250,
  "has_baseline": true
}
```

---

## 3. Root Cause Analysis

### Investigation

The engineering team performed the following analysis:

1. **Sampled misclassified images** — Extracted 200 images from the low-confidence predictions flagged in the last 7 days. Manual review confirmed 73% were AI-generated images incorrectly classified as real.

2. **Identified image sources** — Reverse image search and metadata analysis revealed the misclassified images were primarily generated by DALL-E 3 and Midjourney v6, which were not represented in the training data.

3. **Feature analysis** — Ran the misclassified images through the EfficientNet-B0 feature extractor. The intermediate feature distributions showed significant divergence from the training set distribution, confirming **covariate shift**.

### Root Cause

**The training dataset only contained images from older AI generators** (Stable Diffusion 1.5/2.1, DALL-E 2, Midjourgy v4/v5). The newer generation of AI tools produces images that lack the artifacts the model learned to detect:

- **Texture patterns**: Older generators produced subtle repetitive textures; newer models do not
- **Edge consistency**: Older generators had inconsistent edges around objects; newer models handle this correctly
- **Color space distribution**: The color histogram of newer AI images is closer to real photos
- **Frequency domain**: High-frequency noise patterns used by the model as features are absent in newer generators

### Classification

- **Type**: Data drift (covariate shift)
- **Severity**: High (directly impacts fraud detection capability)
- **Scope**: Affects ~40% of incoming AI-generated images (those from new generators)

---

## 4. Remediation

### Immediate Actions (Day 8-9)

1. **Lowered the decision threshold** from 0.5 to 0.35 to catch more borderline cases. This increased false positives but reduced fraudulent listing leakage as a temporary measure.

2. **Enabled enhanced logging** to capture all low-confidence predictions with associated image hashes for later analysis.

3. **Notified the platform moderation team** to increase manual review of product images in high-risk categories.

### Short-Term Fix (Day 9-15)

1. **Data collection**: Gathered 5,000 new AI-generated images from DALL-E 3, Midjourney v6, and SDXL Turbo across product categories (electronics, fashion, furniture, food).

2. **Dataset augmentation**: Combined new samples with existing training data, maintaining class balance (50/50 real vs AI-generated).

3. **Model retraining**:
   ```bash
   python -m src.training.train --config configs/train_config.yaml
   ```
   - Tracked experiment in MLflow with tag `retrain-drift-2026-q1`
   - New model accuracy: **87.4%** (up from 65.1% on drifted data)

4. **Validation**: Tested specifically against a held-out set of new-generator images:
   - DALL-E 3 detection rate: 84.2%
   - Midjourney v6 detection rate: 81.7%
   - SDXL Turbo detection rate: 86.1%

### Deployment (Day 15-17)

1. **A/B testing**: Deployed the new model to 10% of traffic using a canary deployment strategy.
   - Monitored for 48 hours
   - Confirmed improved detection with no regression on older AI images

2. **Full rollout**: Promoted the new model to 100% of traffic.

3. **Baseline update**: Saved new drift baseline reflecting updated model behavior:
   ```python
   drift_detector.save_baseline(Path("configs/drift_baseline.json"))
   ```

4. **Threshold restored**: Reverted the decision threshold from 0.35 back to 0.5.

### Results

| Metric | Before Fix | After Fix |
|--------|-----------|-----------|
| **Accuracy** | 65.1% | 87.4% |
| **Precision (AI class)** | 58.3% | 88.1% |
| **Recall (AI class)** | 71.2% | 85.6% |
| **Low Confidence Ratio** | 34.8% | 9.7% |

---

## 5. Prevention

### Automated Monitoring Alerts

The following alerting rules were implemented:

| Alert | Condition | Severity | Action |
|-------|-----------|----------|--------|
| Probability Drift | `drift_score > 0.10` | Warning | Slack notification to ML team |
| Probability Drift | `drift_score > 0.15` | Critical | PagerDuty alert to on-call engineer |
| Low Confidence Spike | `low_confidence_ratio > 0.20` | Warning | Log analysis triggered |
| Prediction Ratio Shift | `class_ratio_delta > 0.15` | Warning | Manual review of sample predictions |

### Scheduled Retraining Pipeline

A monthly retraining pipeline was established:

1. **Week 1**: Automated data collection — scrape latest AI-generated images from public datasets and known AI generators
2. **Week 2**: Data validation — automated quality checks and class balance verification
3. **Week 3**: Retraining — triggered via CI/CD pipeline with MLflow experiment tracking
4. **Week 4**: Evaluation and deployment — A/B test and gradual rollout

```
┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐
│  Collect  │--->│ Validate │--->│ Retrain  │--->│  Deploy  │
│   Data    │    │   Data   │    │  Model   │    │  (A/B)   │
└──────────┘    └──────────┘    └──────────┘    └──────────┘
     DVC            pytest         MLflow        Canary 10%
```

### Baseline Management

- **Automatic baseline updates** after each successful model deployment
- **Baseline versioning** tracked in DVC alongside model artifacts
- **Historical baselines** retained for trend analysis

### Additional Safeguards

1. **AI generator tracking**: Maintain a registry of known AI image generators and their release dates. Cross-reference with training data coverage.

2. **Synthetic drift testing**: Periodically inject known out-of-distribution samples to verify the drift detector is functioning correctly.

3. **Model ensemble consideration**: Evaluate using an ensemble of models trained on different generator epochs to improve robustness against future drift.

4. **Human-in-the-loop fallback**: When drift is detected, automatically route low-confidence predictions to human moderators until the model is updated.

---

## Lessons Learned

1. **Drift detection is essential** — Without the `DriftDetector`, the accuracy drop would have gone unnoticed for weeks, allowing thousands of fraudulent listings.

2. **AI evolves fast** — The training data needs to keep pace with advances in generative AI. A static dataset becomes obsolete within months.

3. **Monitoring baselines must be maintained** — The baseline saved during initial deployment was critical for quantifying the drift.

4. **Graceful degradation matters** — Having a threshold adjustment as a quick lever allowed immediate risk reduction while the proper fix was developed.

5. **Retraining pipelines should be ready** — The ability to quickly retrain and deploy prevented this from becoming a prolonged incident.

---

*Document created as part of the MLOps incident response framework.*
*AI Product Photo Detector — M2 MLOps Project, JUNIA 2026*
