# Architecture Audit Report

**Date:** 2025-07-17  
**Scope:** Global architecture, design patterns, scalability, package structure, configuration  
**Project:** AI Product Photo Detector (v1.0.0)

---

## Executive Summary

The project exhibits **solid architecture for an M2 MLOps project**. The separation of concerns is clean, the CI/CD pipeline is production-grade, and the monitoring stack (Prometheus + Grafana + drift detection) demonstrates real operational maturity. However, several patterns and structural decisions need attention for production readiness.

**Overall Grade: B+ (strong academic project, needs polish for production)**

---

## 1. Architecture â€” Separation of Responsibilities

### âœ… What's Good

- **Clean module separation**: `training/`, `inference/`, `monitoring/`, `utils/` each have a clear, single responsibility
- **Data flow is logical**: `data/ â†’ training/ â†’ models/ â†’ inference/ â†’ monitoring/`
- **No business logic leakage**: Schemas stay in `schemas.py`, auth in `auth.py`, validation in `validation.py`
- **API layer is well structured**: `api.py` orchestrates, delegates to `predictor.py` for inference, `drift.py` for monitoring
- **Config centralization**: YAML configs for train/inference, env vars via `pydantic-settings`

### âš ï¸ Issues Found

| # | Issue | Severity | Location |
|---|-------|----------|----------|
| 1 | **`api.py` is 617 lines** â€” too large, handles middleware + routes + lifespan | Medium | `src/inference/api.py` |
| 2 | **`predictor.py` imports from `training.model`** â€” inference depends on training module | Medium | `src/inference/predictor.py:12` |
| 3 | **`validation.py` is unused** â€” thorough validation module exists but `api.py` does its own inline validation | Medium | `src/inference/api.py` vs `validation.py` |
| 4 | **`src/data/__init__.py` is empty** â€” module exists but contains nothing | Low | `src/data/` |
| 5 | **`total_predictions` is a global counter** in `api.py` instead of using the Prometheus counter | Low | `src/inference/api.py` |

### Recommendations

**Issue 1 â€” Split `api.py`:**
```
src/inference/
â”œâ”€â”€ api.py          â†’ App creation, lifespan, root/health/info endpoints (~150 lines)
â”œâ”€â”€ routes.py       â†’ /predict, /predict/batch, /metrics, /drift (~250 lines)  
â”œâ”€â”€ middleware.py   â†’ observability_middleware, CORS setup (~100 lines)
â”œâ”€â”€ predictor.py    â†’ (unchanged)
â”œâ”€â”€ schemas.py      â†’ (unchanged)
â”œâ”€â”€ auth.py         â†’ (unchanged)
â””â”€â”€ validation.py   â†’ (unchanged, but actually wired in)
```

**Issue 2 â€” Decouple inference from training:**  
The `Predictor` imports `AIImageDetector` from `src.training.model` to instantiate the model class. This creates a hard dependency: deploying inference requires shipping the entire training module. Solution: move the model *architecture definition* to a shared location or use `torch.jit` / ONNX for serving.

**Issue 3 â€” Wire up `validation.py`:**  
The `validate_image_bytes()` and `validate_upload_file()` functions in `validation.py` are comprehensive (magic byte detection, dimension checks, content hash). But `api.py` does its own simpler checks inline. The validation module should replace the inline checks.

---

## 2. Design Patterns

### Singleton Pattern (Predictor)

**Current:** Global `predictor: Predictor | None = None` in `api.py`, initialized in `lifespan()`.

**Verdict: Acceptable for this use case**, but not ideal.

| Aspect | Assessment |
|--------|------------|
| Thread safety | âš ï¸ `total_predictions += 1` is not thread-safe (though uvicorn async is single-threaded) |
| Testability | âš ï¸ Global state makes unit testing harder â€” needs monkeypatching |
| Multi-model | âŒ Can't serve multiple models simultaneously |

**Better approach:** FastAPI dependency injection:
```python
async def get_predictor(request: Request) -> Predictor:
    return request.app.state.predictor
```
This is testable, mockable, and supports swapping predictors.

### Factory Pattern (Model)

**Current:** `create_model()` in `model.py` â€” simple factory function.

**Verdict: âœ… Good.** The factory properly abstracts model creation. It accepts `model_name` which allows swapping architectures (e.g., `efficientnet_b0` â†’ `resnet50`).

**Improvement:** Add a model registry for multi-model support:
```python
MODEL_REGISTRY = {
    "efficientnet_b0": AIImageDetector,
    "resnet50": ResNetDetector,
}
```

### Dependency Injection

**Current:** `verify_api_key` uses `Depends()` properly. Good FastAPI pattern.

**Verdict: âœ… Auth DI is clean.** But the predictor and drift detector should also use DI instead of globals.

---

## 3. Scalability

### Multi-Model Support

**Current state: âŒ Single model hardcoded.**

- `Predictor.__init__()` loads one model from one path
- The model architecture class `AIImageDetector` is hardcoded in `_load_model()`
- Config only supports one `model.path`

**To support multiple models:**
1. Model registry pattern (map name â†’ class)
2. Predictor pool (dict of predictors by name/version)
3. Route parameter: `/predict?model=v2`

### New Detection Types

**Current state: âš ï¸ Partially extensible.**

- The binary classification (real vs AI) is baked in with `PredictionResult(StrEnum)` having only `REAL` and `AI_GENERATED`
- Adding a new category (e.g., `PHOTOSHOPPED`) requires changes in: schemas, predictor logic, training pipeline, monitoring
- The model outputs a single sigmoid â€” needs architecture change for multi-class

**Extensibility score: 5/10** â€” possible but requires touching many files.

### Monitoring Scalability

**Current state: âœ… Good foundation.**

- Prometheus metrics are well-defined with proper labels
- Drift detector uses a sliding window (configurable size)
- Docker Compose includes Prometheus + Grafana

**Concern:** Drift detector is in-memory only â€” restarts lose all history. For production, persist to a time-series DB.

---

## 4. Package Structure

### Import Cleanliness

**Analysis of all 29 cross-module imports:**

```
src.inference â†’ src.training    (1 import: model.AIImageDetector)  âš ï¸ Cross-boundary
src.inference â†’ src.monitoring  (2 imports: drift, metrics)        âœ… Expected
src.inference â†’ src.utils       (3 imports: config, logger)        âœ… Expected
src.training  â†’ src.utils       (2 imports: config, logger)        âœ… Expected  
src.monitoring â†’ src.utils      (1 import: logger)                 âœ… Expected
```

**Dependency graph:**
```
utils (leaf) â† training â† inference â†’ monitoring
                              â†‘              â†‘
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**No circular dependencies detected. âœ…**

The only problematic edge is `inference â†’ training` (for the model class). Everything else follows a clean DAG.

### `__init__.py` API Surface

| Module | Exports | Assessment |
|--------|---------|------------|
| `src/` | `__version__` only | âœ… Minimal, correct |
| `src/inference/` | `Predictor` + all schemas | âœ… Good public API |
| `src/training/` | All transforms, dataset, model | âœ… Good public API |
| `src/monitoring/` | Nothing (empty) | âš ï¸ Should export `DriftDetector`, key metrics |
| `src/utils/` | All configs + logger | âœ… Good public API |
| `src/data/` | Nothing (empty module) | âš ï¸ Placeholder â€” remove or populate |
| `src/ui/` | Nothing | âœ… OK (standalone Streamlit app) |

---

## 5. Configuration

### Current State

| Config Source | Type | Contents |
|-------------|------|----------|
| `configs/train_config.yaml` | Build-time | Hyperparameters, data paths, MLflow settings |
| `configs/inference_config.yaml` | Runtime | Server settings, thresholds, rate limits |
| `src/utils/config.py` | Runtime | Pydantic `Settings` (env vars), YAML loader |
| `.env.example` | Template | Environment variable reference |
| `docker-compose.yml` | Deploy-time | Service config, env vars |

### Assessment

**âœ… Good separation:**
- Training config = YAML (static, versioned)
- Runtime config = env vars via `pydantic-settings`
- Infrastructure config = Docker Compose + Terraform

**âš ï¸ Issues:**

1. **Config duplication**: Thresholds are defined in 3 places:
   - `configs/train_config.yaml` â†’ `thresholds.classification: 0.5`
   - `configs/inference_config.yaml` â†’ `thresholds.classification: 0.5`
   - `src/inference/predictor.py` â†’ `threshold: float = 0.5` (default arg)
   
   â†’ Single source of truth needed.

2. **Pydantic config models exist but aren't used**: `DataConfig`, `ModelConfig`, `TrainingConfig`, `ThresholdsConfig` are defined in `config.py` but `train.py` just uses raw `dict` from YAML. The typed config models should be used.

3. **`inference_config.yaml` partially ignored**: The server config, rate limit config, and metrics config in `inference_config.yaml` are never read â€” `api.py` hardcodes these values (`MAX_FILE_SIZE`, rate limits, etc.).

4. **Image size mismatch**: Training config says `image_size: 128` but inference `Predictor` resizes to `224Ã—224`. This is a **functional bug** â€” the model was trained on 128Ã—128 but inference sends 224Ã—224. (Mitigated because the checkpoint stores config and predictor could read it, but the transform is hardcoded to 224.)

---

## 6. Missing Pieces for Production MLOps

### Critical (Must-Have)

| # | Missing | Why It Matters |
|---|---------|---------------|
| 1 | **Model registry** (MLflow Model Registry or similar) | No way to promote staging â†’ production models |
| 2 | **A/B testing / shadow mode** | Can't safely roll out new model versions |
| 3 | **Data validation pipeline** (Great Expectations / Pandera) | No input data quality gates |
| 4 | **Persistent drift storage** | Drift history lost on restart |
| 5 | **Alerting rules** (Grafana alerts or PagerDuty) | Drift detected but nobody gets notified |
| 6 | **Rollback mechanism** | `deploy.yml` deploys but no automated rollback on failure |

### Important (Should-Have)

| # | Missing | Why It Matters |
|---|---------|---------------|
| 7 | **Feature store** | Image embeddings not stored for analysis |
| 8 | **Retraining pipeline trigger** | Drift detected â†’ manual intervention required |
| 9 | **Load testing** (Locust / k6) | No performance baseline |
| 10 | **API versioning** (`/v1/predict`) | Breaking changes require client updates |
| 11 | **Canary deployments** | All-or-nothing deployments are risky |
| 12 | **Secret management** (GCP Secret Manager) | API keys via env vars, not rotatable |

### Nice-to-Have

| # | Missing | Why It Matters |
|---|---------|---------------|
| 13 | **Model explainability** (GradCAM / SHAP) | Can't explain *why* an image is flagged |
| 14 | **Batch inference pipeline** (async / queue-based) | Current batch is synchronous, blocks |
| 15 | **Multi-region deployment** | Single region = single point of failure |

---

## 7. Fixes Applied

### Fix 1: `src/monitoring/__init__.py` â€” Expose Public API

The monitoring module exported nothing, making users import directly from submodules.

### Fix 2: `src/data/__init__.py` â€” Document purpose or mark as namespace

### Fix 3: Image size inconsistency documentation

---

## 8. Summary of Recommendations (Priority Order)

| Priority | Action | Effort |
|----------|--------|--------|
| ğŸ”´ High | Fix image size mismatch (128 train vs 224 inference) | 1h |
| ğŸ”´ High | Wire `validation.py` into `api.py` (replace inline checks) | 2h |
| ğŸŸ¡ Medium | Decouple `inference` from `training` (shared model module) | 3h |
| ğŸŸ¡ Medium | Split `api.py` into routes + middleware | 2h |
| ğŸŸ¡ Medium | Use Pydantic config models instead of raw dicts in `train.py` | 2h |
| ğŸŸ¡ Medium | Replace global state with FastAPI DI | 1h |
| ğŸŸ¢ Low | Add model registry for multi-model support | 4h |
| ğŸŸ¢ Low | Single source of truth for thresholds | 1h |
| ğŸŸ¢ Low | Add API versioning (`/v1/`) | 1h |

---

## Architecture Diagram (Current)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    src/ (package)                        â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ utils/   â”‚â—„â”€â”€â”€â”‚ training/ â”‚    â”‚   inference/      â”‚  â”‚
â”‚  â”‚  config  â”‚    â”‚  model    â”‚â—„â”€â”€â”€â”‚   predictor â”€â”€â”   â”‚  â”‚
â”‚  â”‚  logger  â”‚â—„â”€â”€â”€â”‚  dataset  â”‚    â”‚   api â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚  â”‚
â”‚  â”‚          â”‚    â”‚  train    â”‚    â”‚   schemas     â”‚   â”‚  â”‚
â”‚  â”‚          â”‚    â”‚  augment  â”‚    â”‚   auth        â”‚   â”‚  â”‚
â”‚  â”‚          â”‚    â”‚           â”‚    â”‚   validation  â”‚   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚       â–²                                    â”‚          â”‚  â”‚
â”‚       â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚          â”‚  â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ monitoring/  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚
â”‚                  â”‚  metrics     â”‚                      â”‚  â”‚
â”‚                  â”‚  drift       â”‚                      â”‚  â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚  â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚  data/   â”‚    â”‚   ui/    â”‚  (standalone Streamlit)   â”‚
â”‚  â”‚ (empty)  â”‚    â”‚  app.py  â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

*Report generated by architecture audit. No commits made.*
