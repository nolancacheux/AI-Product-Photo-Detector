# Docker Compose for AI Product Photo Detector
# Usage: docker compose up -d

services:
  # Inference API (Cloud Run compatible)
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
    image: ai-product-detector:1.0.0
    container_name: ai-detector-api
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
      - ./configs:/app/configs:ro
    environment:
      - PORT=8080
      - MODEL_PATH=/app/models/checkpoints/best_model.pt
      - LOG_LEVEL=INFO
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    networks:
      - detector-network

  # Streamlit UI
  ui:
    build:
      context: .
      dockerfile: docker/Dockerfile
    image: ai-product-detector:1.0.0
    container_name: ai-detector-ui
    command: ["streamlit", "run", "src/ui/app.py", "--server.port=8501", "--server.address=0.0.0.0"]
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://api:8080
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - detector-network

  # MLflow Tracking Server
  mlflow:
    image: python:3.11-slim
    container_name: ai-detector-mlflow
    command: >
      bash -c "pip install mlflow && mlflow server 
      --host 0.0.0.0 
      --port 5000 
      --backend-store-uri sqlite:///mlflow.db 
      --default-artifact-root /mlflow/artifacts"
    ports:
      - "5000:5000"
    volumes:
      - mlflow-data:/mlflow
    restart: unless-stopped
    networks:
      - detector-network

networks:
  detector-network:
    driver: bridge

volumes:
  mlflow-data:
